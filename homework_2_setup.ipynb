{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70225573-6a63-47b5-9e4e-3311014fe228",
   "metadata": {},
   "source": [
    "# 1. PCA for Network Intrusion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05100ad6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T13:44:11.395800Z",
     "start_time": "2025-05-27T13:44:11.389617Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_kdd_data(data_dir=\"./datasets/kdd_balanced\"):\n",
    "    \"\"\"\n",
    "    Load the balanced dataset from Parquet format (ultra-fast loading).\n",
    "    \n",
    "    Returns:\n",
    "        D_balanced: Feature matrix (numpy array)\n",
    "        is_normal_balanced: Boolean labels (numpy array)  \n",
    "        original_indices: Original indices from full dataset (numpy array)\n",
    "        df_balanced: Original dataframe subset (if available)\n",
    "        metadata: Dataset metadata\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    \n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    # Check if required files exist\n",
    "    required_files = [\"balanced_dataset.parquet\", \"metadata.json\"]\n",
    "    missing_files = [f for f in required_files if not (data_path / f).exists()]\n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(f\"Required files not found in {data_path}: {missing_files}\")\n",
    "    \n",
    "    # Load main dataset (this is very fast with Parquet)\n",
    "    df_main = pd.read_parquet(data_path / \"balanced_dataset.parquet\")\n",
    "    \n",
    "    # Extract components\n",
    "    feature_cols = [col for col in df_main.columns if col.startswith('feature_')]\n",
    "    D_balanced = df_main[feature_cols].values\n",
    "    is_normal_balanced = df_main['is_normal'].values\n",
    "    original_indices = df_main['original_index'].values\n",
    "    \n",
    "    # Load original dataframe if available\n",
    "    df_balanced = None\n",
    "    if (data_path / \"original_data_balanced.parquet\").exists():\n",
    "        df_balanced = pd.read_parquet(data_path / \"original_data_balanced.parquet\")\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(data_path / \"metadata.json\", 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"Balanced dataset loaded from {data_path} (Parquet format)\")\n",
    "    print(f\"  Features: {D_balanced.shape}\")\n",
    "    print(f\"  Normal: {metadata['n_normal']:,}, Intrusion: {metadata['n_intrusion']:,}\")\n",
    "    \n",
    "    return D_balanced, is_normal_balanced, original_indices, df_balanced, metadata\n",
    "\n",
    "def compute_pca_from_intrusion_data(D_intrusion, r):\n",
    "    \"\"\"\n",
    "    Compute PCA using SVD on intrusion data with rank r.\n",
    "    \n",
    "    Key insight: We train PCA on INTRUSION data, so normal data\n",
    "    will have higher reconstruction error in this learned space.\n",
    "    \n",
    "    Returns:\n",
    "        X: Principal components (learned from intrusion data)\n",
    "        mu_intrusion: Mean vector of intrusion data\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def project_to_low_dimensional_space(D, X, mu_intrusion):\n",
    "    \"\"\"\n",
    "    Project any data points into the low-dimensional space defined by\n",
    "    intrusion-trained PCA components.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def reconstruct_from_low_dimensional(Y, X, mu_intrusion):\n",
    "    \"\"\"Reconstruct data points from low-dimensional coordinates.\"\"\"\n",
    "\n",
    "\n",
    "def compute_reconstruction_error(original, reconstructed):\n",
    "    \"\"\"Compute L2 reconstruction error.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65fa2795e898f5d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T13:44:18.586799Z",
     "start_time": "2025-05-27T13:44:11.405201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset loaded from datasets/kdd_balanced (Parquet format)\n",
      "  Features: (97378, 118)\n",
      "  Normal: 97,278, Intrusion: 100\n",
      "Low-dim coords of sample 7 (first three):\n",
      "z₀ = -0.099082\n",
      "z₁ = -0.993120\n",
      "z₂ = -0.062970\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "D_balanced, is_normal_balanced, original_indices, _, metadata = load_kdd_data()\n",
    "\n",
    "# Your code here\n",
    "pca = PCA(n_components=10, random_state=0)\n",
    "Z = pca.fit_transform(D_balanced) \n",
    "\n",
    "z = Z[7]\n",
    "z0, z1, z2 = z[0], z[1], z[2]\n",
    "\n",
    "print(f\"Low-dim coords of sample 7 (first three):\\n\"\n",
    "      f\"z₀ = {z0:.6f}\\n\"\n",
    "      f\"z₁ = {z1:.6f}\\n\"\n",
    "      f\"z₂ = {z2:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2489cc87",
   "metadata": {},
   "source": [
    "# 2. K-means Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d77a95d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,2) (4,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 130\u001b[39m\n\u001b[32m    127\u001b[39m r = \u001b[32m3\u001b[39m\n\u001b[32m    128\u001b[39m l = \u001b[32m10\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m X_init = \u001b[43minit_centroids_greedy_pp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m X_final, Y_final = kmeans(data, r, X_init)\n\u001b[32m    134\u001b[39m labels_pred = np.argmax(Y_final, axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36minit_centroids_greedy_pp\u001b[39m\u001b[34m(D, r, l)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m indexes:\n\u001b[32m     75\u001b[39m     X_temp = np.hstack([X, D[\u001b[38;5;28mid\u001b[39m][:, \u001b[38;5;28;01mNone\u001b[39;00m]])\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     total = \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdistance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_temp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     costs.append(total)\n\u001b[32m     80\u001b[39m costs = np.array(costs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m \u001b[38;5;129;01min\u001b[39;00m indexes:\n\u001b[32m     75\u001b[39m     X_temp = np.hstack([X, D[\u001b[38;5;28mid\u001b[39m][:, \u001b[38;5;28;01mNone\u001b[39;00m]])\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     total = \u001b[38;5;28msum\u001b[39m(\u001b[43mdistance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mD\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_temp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n))\n\u001b[32m     78\u001b[39m     costs.append(total)\n\u001b[32m     80\u001b[39m costs = np.array(costs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mdistance\u001b[39m\u001b[34m(v, X)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdistance\u001b[39m(v, X):\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Return minimum distance to any column in X\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.min(np.sum((\u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m)**\u001b[32m2\u001b[39m, axis=\u001b[32m1\u001b[39m))\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (4,2) (4,) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def distance(v, X):\n",
    "    # Return minimum distance to any column in X\n",
    "    diff = X - v[:, None]\n",
    "    sq = np.sum(diff**2, axis=0)\n",
    "\n",
    "    return np.min(sq)\n",
    "\n",
    "\n",
    "def load_iris_data(path=Path(\"datasets/iris/iris.csv\")):\n",
    "    df = pd.read_csv(path)\n",
    "    data = df.iloc[:, :-1].values\n",
    "    labels = df['target'].values\n",
    "    return data, labels\n",
    "\n",
    "def print_iris_info(data, labels):\n",
    "    n_samples, n_features = data.shape\n",
    "    n_classes = len(np.unique(labels))\n",
    "    feature_names = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "\n",
    "    print(f\"Number of samples: {n_samples}\")\n",
    "    print(f\"Number of features: {n_features}\")\n",
    "    print(f\"Number of classes: {n_classes}\")\n",
    "    print(f\"Feature names: {feature_names}\")\n",
    "    print(f\"Class distribution:\")\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    for cls, count in zip(unique, counts):\n",
    "        print(f\"  Class {cls}: {count} samples\")\n",
    "\n",
    "def init_centroids_greedy_pp(D,r,l=10):\n",
    "    '''\n",
    "        :param r: (int) number of centroids (clusters)\n",
    "        :param D: (np-array) the data matrix\n",
    "        :param l: (int) number of centroid candidates in each step\n",
    "        :return: (np-array) 'X' the selected centroids from the dataset\n",
    "    '''   \n",
    "    rng =  np.random.default_rng(seed=RANDOM_SEED) # use this random generator to sample the candidates (sampling according to given probabilities can be done via rng.choice(..))\n",
    "    n,d = D.shape\n",
    "\n",
    "    # Your code here\n",
    "\n",
    "\n",
    "    # Sample l candidates with a uniform random distribution\n",
    "    indexes = rng.integers(low=0, high=n, size=l)\n",
    "\n",
    "    cands = D[indexes]\n",
    "\n",
    "    diff =  cands[:, None, :] - D[None, :, :]\n",
    "    sqdist = np.sum(diff**2, axis=2)\n",
    "    cost = np.sum(sqdist, axis=1)\n",
    "\n",
    "    # Find the best candidate\n",
    "    best_pos = np.argmin(cost)\n",
    "    i = indexes[best_pos]\n",
    "\n",
    "    X = D[i][:, None]\n",
    "\n",
    "    s = 2\n",
    "    while s <= r:\n",
    "        dist_vec = np.array([distance(D[j], X) for j in range(n)])\n",
    "        probs = dist_vec / dist_vec.sum()\n",
    "\n",
    "        indexes = rng.choice(n, size=l, replace=False, p=probs)\n",
    "        cands = D[indexes]\n",
    "\n",
    "        costs = []\n",
    "        for id in indexes:\n",
    "            X_temp = np.hstack([X, D[id][:, None]])\n",
    "\n",
    "            total = sum(distance(D[j], X_temp) for j in range(n))\n",
    "            costs.append(total)\n",
    "\n",
    "        costs = np.array(costs)\n",
    "        best_pos = np.argmin(costs)\n",
    "        i = indexes[best_pos]\n",
    "\n",
    "        X = np.hstack([X, D[i][:, None]]) \n",
    "\n",
    "        s += 1\n",
    "\n",
    "    return X\n",
    "\n",
    "# K-means implementation from the lecture slides\n",
    "def RSS(D,X,Y):\n",
    "    return np.sum((D- Y@X.T)**2)\n",
    "    \n",
    "def getY(labels):\n",
    "    Y = np.eye(max(labels)+1)[labels]\n",
    "    return Y\n",
    "    \n",
    "def update_centroid(D,Y):\n",
    "    cluster_sizes = np.diag(Y.T@Y).copy()\n",
    "    cluster_sizes[cluster_sizes==0]=1\n",
    "    return D.T@Y/cluster_sizes\n",
    "    \n",
    "def update_assignment(D,X):\n",
    "    dist = np.sum((np.expand_dims(D,2) - X)**2,1)\n",
    "    labels = np.argmin(dist,1)\n",
    "    return getY(labels)\n",
    "    \n",
    "def kmeans(D,r, X_init, epsilon=0.00001, t_max=10000):\n",
    "    X = X_init.copy()\n",
    "    Y = update_assignment(D,X)\n",
    "    rss_old = RSS(D,X,Y) +2*epsilon\n",
    "    t=0\n",
    "    \n",
    "    #Looping as long as difference of objective function values is larger than epsilon\n",
    "    while rss_old - RSS(D,X,Y) > epsilon and t < t_max-1:\n",
    "        rss_old = RSS(D,X,Y)\n",
    "        X = update_centroid(D,Y)\n",
    "        Y = update_assignment(D,X)\n",
    "        t+=1\n",
    "    print(t,\"iterations\")\n",
    "    return X,Y\n",
    "\n",
    "\n",
    "data, labels = load_iris_data()\n",
    "\n",
    "# Your code here\n",
    "r = 3\n",
    "l = 10\n",
    "\n",
    "X_init = init_centroids_greedy_pp(data, r, l)\n",
    "\n",
    "X_final, Y_final = kmeans(data, r, X_init)\n",
    "\n",
    "labels_pred = np.argmax(Y_final, axis=1)\n",
    "\n",
    "n, d = data.shape\n",
    "\n",
    "mse = RSS(data, X_final, Y_final) / (n * d)\n",
    "print(\"MSE:\", mse)\n",
    "\n",
    "nmi = normalized_mutual_info_score(labels, labels_pred)\n",
    "print(\"NMI:\", nmi)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b0be9",
   "metadata": {},
   "source": [
    "# 3. Netflix Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8999edd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T12:59:37.201807Z",
     "start_time": "2025-05-21T12:59:36.986301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Part 3a (λ=0.1) ---\n",
      "Average squared error (observed): 0.0939\n",
      "\n",
      "Estimated (raw) ratings for user #1:\n",
      " • Jumanji (1995): 3.75983\n",
      " • Monty Python and the Holy Grail (1975): 4.64207\n",
      " • Matrix, The (1999): 4.74685\n",
      " • Fight Club (1999): 4.77567\n",
      "\n",
      "\n",
      "--- Part 3b: Effect of regularization ---\n",
      "λ = 0.01\n",
      " • missing‐entry mean:    3.3258\n",
      " • missing‐entry variance: 6.1615\n",
      " • observed‐entry error:   0.0926\n",
      " • missing outside [0.5,5]: 14619/54810 (26.67%)\n",
      "\n",
      "λ = 0.10\n",
      " • missing‐entry mean:    3.0315\n",
      " • missing‐entry variance: 3.4139\n",
      " • observed‐entry error:   0.0939\n",
      " • missing outside [0.5,5]: 10692/54810 (19.51%)\n",
      "\n",
      "λ = 0.50\n",
      " • missing‐entry mean:    3.3463\n",
      " • missing‐entry variance: 1.3152\n",
      " • observed‐entry error:   0.1017\n",
      " • missing outside [0.5,5]: 4087/54810 (7.46%)\n",
      "\n",
      "--- Part 3c: Monty Python and the Holy Grail (1975) ---\n",
      "Rating MP_λ=0.01 = 4.78720\n",
      "Rating MP_λ=0.50 = 5.20685\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Data loading & filtering ---\n",
    "def load_ratings_data_pandas(data_dir=\"datasets/ml-latest-small/\"):\n",
    "    data_dir = Path(data_dir)\n",
    "    return pd.read_csv(data_dir / 'ratings.csv', sep=',')\n",
    "\n",
    "def load_movies_data_pandas(data_dir=\"datasets/ml-latest-small/\"):\n",
    "    data_dir = Path(data_dir)\n",
    "    return pd.read_csv(data_dir / 'movies.csv')\n",
    "\n",
    "def filter_data(ratings_df, movies_df):\n",
    "    R = ratings_df.pivot(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "    keep_movie = (R != 0).sum(axis=0) > 100\n",
    "    R = R.loc[:, keep_movie]\n",
    "    movies_df = movies_df[movies_df.movieId.isin(R.columns)]\n",
    "    keep_user = (R != 0).sum(axis=1) >= 5\n",
    "    R = R.loc[keep_user, :]\n",
    "    return R, movies_df\n",
    "\n",
    "ratings_df, movies_df = filter_data(\n",
    "    load_ratings_data_pandas(),\n",
    "    load_movies_data_pandas()\n",
    ")\n",
    "D = ratings_df.to_numpy()        # shape: (n_users, n_movies)\n",
    "mask_obs = D != 0\n",
    "n_users, n_movies = D.shape\n",
    "\n",
    "# --- 2. Block‐coordinate descent for matrix completion ---\n",
    "def matrix_completion(D, n_features, t_max=100, lambd=0.1):\n",
    "    np.random.seed(0)\n",
    "    n_users, n_movies = D.shape\n",
    "    X = np.random.normal(size=(n_movies, n_features))\n",
    "    Y = np.random.normal(size=(n_users, n_features))\n",
    "    \n",
    "    for _ in range(t_max):\n",
    "        # ---- swap the usual order: update X first ----\n",
    "        for j in range(n_movies):\n",
    "            mask_j = D[:, j] != 0\n",
    "            Yj = Y[mask_j]\n",
    "            if Yj.size == 0:\n",
    "                continue\n",
    "            A = Yj.T @ Yj + lambd * np.eye(n_features)\n",
    "            b = Yj.T @ D[mask_j, j]\n",
    "            X[j, :] = np.linalg.solve(A, b)\n",
    "        \n",
    "        # then update Y\n",
    "        for i in range(n_users):\n",
    "            mask_i = D[i, :] != 0\n",
    "            Xi = X[mask_i]\n",
    "            if Xi.size == 0:\n",
    "                continue\n",
    "            A = Xi.T @ Xi + lambd * np.eye(n_features)\n",
    "            b = Xi.T @ D[i, mask_i]\n",
    "            Y[i, :] = np.linalg.solve(A, b)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "# --- 3a: λ=0.1, r=20, t_max=100 ---\n",
    "r       = 20\n",
    "t_max   = 100\n",
    "lam_3a  = 0.1\n",
    "\n",
    "X, Y    = matrix_completion(D, n_features=r, t_max=t_max, lambd=lam_3a)\n",
    "pred    = Y @ X.T   # raw predictions\n",
    "\n",
    "# 3a(i): average squared error on observed entries\n",
    "avg_sq_error = ((D[mask_obs] - pred[mask_obs])**2).mean()\n",
    "\n",
    "# 3a(ii): estimated ratings for the first user\n",
    "df_preds = pd.DataFrame({\n",
    "    'movieId':    ratings_df.columns,\n",
    "    'pred_rating': pred[0]\n",
    "}).merge(movies_df[['movieId','title']], on='movieId')\n",
    "\n",
    "target_titles = [\n",
    "    \"Jumanji (1995)\",\n",
    "    \"Fight Club (1999)\",\n",
    "    \"Matrix, The (1999)\",\n",
    "    \"Monty Python and the Holy Grail (1975)\"\n",
    "]\n",
    "df_filtered      = df_preds[df_preds.title.isin(target_titles)]\n",
    "first_user_preds = df_filtered.set_index('title')['pred_rating']\n",
    "\n",
    "print(f\"--- Part 3a (λ={lam_3a}) ---\")\n",
    "print(f\"Average squared error (observed): {avg_sq_error:.4f}\\n\")\n",
    "print(\"Estimated (raw) ratings for user #1:\")\n",
    "for title, rating in first_user_preds.items():\n",
    "    print(f\" • {title}: {rating:.5f}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "# --- Prep for part 3c ---\n",
    "mp_title = \"Monty Python and the Holy Grail (1975)\"\n",
    "mp_mid   = movies_df[movies_df.title == mp_title]['movieId'].iloc[0]\n",
    "mp_col   = list(ratings_df.columns).index(mp_mid)\n",
    "mp_ratings = {}\n",
    "\n",
    "# --- 3b & 3c: sweep λ ∈ {0.01, 0.1, 0.5} ---\n",
    "print(\"--- Part 3b: Effect of regularization ---\")\n",
    "for lam in [0.01, 0.1, 0.5]:\n",
    "    Xb, Yb = matrix_completion(D, n_features=r, t_max=t_max, lambd=lam)\n",
    "    P      = Yb @ Xb.T\n",
    "\n",
    "    miss_vals = P[~mask_obs]\n",
    "    mean_miss = miss_vals.mean()\n",
    "    var_miss  = miss_vals.var()\n",
    "    err_obs   = ((D[mask_obs] - P[mask_obs])**2).mean()\n",
    "    n_out     = np.logical_or(miss_vals < 0.5, miss_vals > 5.0).sum()\n",
    "    frac_out  = n_out / miss_vals.size\n",
    "\n",
    "    if lam in (0.01, 0.5):\n",
    "        mp_ratings[lam] = P[0, mp_col]\n",
    "\n",
    "    print(f\"λ = {lam:.2f}\")\n",
    "    print(f\" • missing‐entry mean:    {mean_miss:.4f}\")\n",
    "    print(f\" • missing‐entry variance: {var_miss:.4f}\")\n",
    "    print(f\" • observed‐entry error:   {err_obs:.4f}\")\n",
    "    print(f\" • missing outside [0.5,5]: {n_out}/{miss_vals.size} ({frac_out:.2%})\\n\")\n",
    "\n",
    "# --- 3c: Monty Python & the Holy Grail predictions ---\n",
    "print(\"--- Part 3c: Monty Python and the Holy Grail (1975) ---\")\n",
    "print(f\"Rating MP_λ=0.01 = {mp_ratings[0.01]:.5f}\")\n",
    "print(f\"Rating MP_λ=0.50 = {mp_ratings[0.5]:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3e83f2b82e3dccf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T12:59:37.485146Z",
     "start_time": "2025-05-21T12:59:37.387487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Summary\n",
      "----------------\n",
      "Users: 556\n",
      "Movies: 134\n",
      "Total Ratings: 19694\n",
      "Data Density: 0.2643 (fraction of observed ratings)\n"
     ]
    }
   ],
   "source": [
    "ratings = load_ratings_data(\"datasets/ml-latest-small\", print_summary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81460c82",
   "metadata": {},
   "source": [
    "# 4. Image Classification With Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d56288",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN Embedding Space Visualization\n",
    "\n",
    "This educational module demonstrates:\n",
    "- ResNet-style architecture with skip connections\n",
    "- Embedding space learning for visualization\n",
    "- Domain transfer between MNIST and Fashion-MNIST\n",
    "- Decision boundary visualization\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional, Callable\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration parameters for the model and training.\"\"\"\n",
    "\n",
    "    # Model architecture\n",
    "    embedding_dim: int = 2\n",
    "    num_classes: int = 10\n",
    "\n",
    "    # Training hyperparameters\n",
    "    learning_rate: float = 0.9 \n",
    "    momentum: float = 0.9\n",
    "    weight_decay: float = 5e-4\n",
    "    batch_size: int = 128\n",
    "    epochs: int = 5\n",
    "    dropout_rate_1: float = 0.9\n",
    "    dropout_rate_2: float = 0.9\n",
    "\n",
    "    # Visualization\n",
    "    viz_samples: int = 100\n",
    "    viz_zoom: float = 0.7\n",
    "    grid_resolution: float = 0.1 \n",
    "    \n",
    "    # Paths\n",
    "    checkpoint_dir: Path = Path(\"checkpoint\")\n",
    "    model_filename: str = \"embedding_model.pth\"\n",
    "\n",
    "    @property\n",
    "    def device(self) -> str:\n",
    "        \"\"\"Get the appropriate device for computation.\"\"\"\n",
    "        return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with skip connections and grouped convolutions.\n",
    "\n",
    "    Implements: output = input + F(input)\n",
    "    where F is a residual function composed of BatchNorm→ReLU→Conv layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, groups: int = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        groups = min(groups, min(in_channels, out_channels))\n",
    "\n",
    "        # Main convolution pathway\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                              padding=\"same\", groups=groups)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size,\n",
    "                              padding=\"same\", groups=min(groups, out_channels))\n",
    "\n",
    "        # Skip connection (identity or dimension adjustment)\n",
    "        self.skip_connection = (\n",
    "            nn.Identity() if in_channels == out_channels\n",
    "            else nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=\"same\")\n",
    "        )\n",
    "\n",
    "        # Pre-activation normalization layers\n",
    "        self.norm1 = nn.BatchNorm2d(in_channels)\n",
    "        self.norm2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass implementing residual connection.\"\"\"\n",
    "        identity = self.skip_connection(x)\n",
    "\n",
    "        # Residual pathway: BatchNorm → ReLU → Conv → BatchNorm → ReLU → Conv\n",
    "        out = self.conv1(self.relu(self.norm1(x)))\n",
    "        out = self.conv2(self.relu(self.norm2(out)))\n",
    "\n",
    "        return identity + out\n",
    "\n",
    "\n",
    "class EmbeddingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN that maps input images to low-dimensional embedding space.\n",
    "\n",
    "    Uses global average pooling instead of flattening to reduce overfitting\n",
    "    and make the model robust to different input sizes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int, dropout_rate_1: float, dropout_rate_2: float):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initial feature extraction\n",
    "        self.initial_conv = nn.Conv2d(1, 32, kernel_size=5, padding=\"same\")\n",
    "        self.initial_norm = nn.BatchNorm2d(32)\n",
    "\n",
    "        # First residual block set (32 channels, groups=2)\n",
    "        self.res_block1 = ResidualBlock(32, 32, kernel_size=3, groups=2)\n",
    "        self.res_block2 = ResidualBlock(32, 32, kernel_size=3, groups=2)\n",
    "\n",
    "        # Spatial downsampling\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.norm_after_pool = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Second residual block set (64 channels, groups=4)\n",
    "        self.res_block3 = ResidualBlock(32, 64, kernel_size=3, groups=4)\n",
    "        self.res_block4 = ResidualBlock(64, 64, kernel_size=3, groups=4)\n",
    "\n",
    "        # Final processing\n",
    "        self.final_norm = nn.BatchNorm1d(64)\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, embedding_dim)\n",
    "\n",
    "        # Regularization\n",
    "        self.dropout1 = nn.Dropout(dropout_rate_1)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate_2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass mapping images to embedding space.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, 1, 28, 28)\n",
    "\n",
    "        Returns:\n",
    "            Embedding tensor of shape (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        out = F.relu(self.initial_norm(self.initial_conv(x)))\n",
    "\n",
    "        # out.shape = ? (tensor shape 1)\n",
    "\n",
    "        # First round of residual blocks\n",
    "        out = self.res_block2(self.res_block1(out))\n",
    "\n",
    "        # out.shape = ? (tensor shape 2)\n",
    "\n",
    "        # Pooling\n",
    "        out = self.norm_after_pool(self.pool(out))\n",
    "\n",
    "        # out.shape = ? (tensor shape 3)\n",
    "\n",
    "        # Second round of residual blocks\n",
    "        out = self.res_block4(self.res_block3(out))\n",
    "\n",
    "        # out.shape = ? (tensor shape 4)\n",
    "\n",
    "        # Global average pooling\n",
    "        out = torch.mean(out, dim=(-1, -2))\n",
    "        out = self.final_norm(out)\n",
    "\n",
    "        # out.shape = ? (tensor shape 5)\n",
    "\n",
    "        # Map to embedding space\n",
    "        out = self.dropout1(out)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "    \"\"\"Complete model combining embedding network with classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int, num_classes: int, config: Config):\n",
    "        super().__init__()\n",
    "        self.embedding_net = EmbeddingNetwork(\n",
    "            embedding_dim, config.dropout_rate_1, config.dropout_rate_2\n",
    "        )\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes, bias=True)\n",
    "\n",
    "        nn.init.normal_(self.classifier.weight, 0, 0.01)\n",
    "        nn.init.constant_(self.classifier.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for training and evaluation.\"\"\"\n",
    "        embeddings = self.embedding_net(x)\n",
    "        return self.classifier(embeddings)\n",
    "\n",
    "    def get_embeddings(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Extract embeddings for visualization.\"\"\"\n",
    "        return self.embedding_net(x)\n",
    "\n",
    "    def get_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get class probabilities for confidence visualization.\"\"\"\n",
    "        embeddings = self.embedding_net(x)\n",
    "        return F.softmax(self.classifier(embeddings), dim=1)\n",
    "\n",
    "\n",
    "def create_data_loaders(dataset_class, config: Config) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Create training and test data loaders.\n",
    "\n",
    "    Args:\n",
    "        dataset_class: torchvision dataset class (MNIST or FashionMNIST)\n",
    "        config: Configuration object\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_loader, test_loader)\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # MNIST standard values\n",
    "    ])\n",
    "\n",
    "    # Training data\n",
    "    train_dataset = dataset_class(root='./data', train=True, download=True, transform=transform)\n",
    "    if config.num_classes < 10:\n",
    "        mask = train_dataset.targets < config.num_classes\n",
    "        train_dataset.targets = train_dataset.targets[mask]\n",
    "        train_dataset.data = train_dataset.data[mask]\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2\n",
    "    )\n",
    "\n",
    "    # Test data\n",
    "    test_dataset = dataset_class(root='./data', train=False, download=True, transform=transform)\n",
    "    if config.num_classes < 10:\n",
    "        mask = test_dataset.targets < config.num_classes\n",
    "        test_dataset.targets = test_dataset.targets[mask]\n",
    "        test_dataset.data = test_dataset.data[mask]\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EpochMetrics:\n",
    "    \"\"\"Container for epoch training/evaluation metrics.\"\"\"\n",
    "    accuracy: float\n",
    "    avg_confidence: float\n",
    "    avg_loss: float\n",
    "    total_samples: int\n",
    "    elapsed_time: Optional[float] = None\n",
    "\n",
    "\n",
    "def compute_batch_metrics(logits: torch.Tensor, targets: torch.Tensor, loss: torch.Tensor) -> Tuple[int, float, int]:\n",
    "    \"\"\"\n",
    "    Compute metrics for a single batch.\n",
    "\n",
    "    Args:\n",
    "        logits: Model output logits\n",
    "        targets: Ground truth labels\n",
    "        loss: Computed loss for the batch\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (correct_predictions, total_confidence, batch_size)\n",
    "    \"\"\"\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    confidences, predictions = probabilities.max(1)\n",
    "\n",
    "    correct_predictions = predictions.eq(targets).sum().item()\n",
    "    total_confidence = confidences.sum().item()\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    return correct_predictions, total_confidence, batch_size\n",
    "\n",
    "\n",
    "def run_epoch(\n",
    "    model: nn.Module,\n",
    "    criterion,\n",
    "    data_loader: DataLoader,\n",
    "    device: str,\n",
    "    optimizer=None,\n",
    "    is_training: bool = True\n",
    ") -> EpochMetrics:\n",
    "    \"\"\"\n",
    "    Run one epoch of training or evaluation.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        criterion: Loss function\n",
    "        data_loader: Data loader\n",
    "        device: Device to run on\n",
    "        optimizer: Optimizer (required if is_training=True)\n",
    "        is_training: Whether to run in training mode\n",
    "\n",
    "    Returns:\n",
    "        EpochMetrics containing all computed metrics\n",
    "    \"\"\"\n",
    "    if is_training:\n",
    "        if optimizer is None:\n",
    "            raise ValueError(\"Optimizer required for training mode\")\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_confidence = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    context_manager = torch.no_grad() if not is_training else torch.enable_grad()\n",
    "\n",
    "    with context_manager:\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if is_training:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                print(\"Warning: NaN loss detected\")\n",
    "\n",
    "            if is_training:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Compute batch metrics (always without gradients for metrics)\n",
    "            with torch.no_grad():\n",
    "                batch_correct, batch_confidence, batch_size = compute_batch_metrics(logits, targets, loss)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_correct += batch_correct\n",
    "                total_confidence += batch_confidence\n",
    "                total_samples += batch_size\n",
    "\n",
    "    # Calculate final metrics\n",
    "    if total_samples == 0:\n",
    "        print(\"Warning: No samples processed\")\n",
    "        return EpochMetrics(0, 0, float('inf'), 0, time.time() - start_time)\n",
    "\n",
    "    accuracy = 100.0 * total_correct / total_samples\n",
    "    avg_confidence = 100.0 * total_confidence / total_samples\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    return EpochMetrics(\n",
    "        accuracy=accuracy,\n",
    "        avg_confidence=avg_confidence,\n",
    "        avg_loss=avg_loss,\n",
    "        total_samples=total_samples,\n",
    "        elapsed_time=elapsed_time\n",
    "    )\n",
    "\n",
    "\n",
    "def train_epoch(model: nn.Module, criterion, optimizer, data_loader: DataLoader, device: str) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Train model for one epoch.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (accuracy, average_confidence)\n",
    "    \"\"\"\n",
    "    metrics = run_epoch(model, criterion, data_loader, device, optimizer, is_training=True)\n",
    "\n",
    "    print(f'Train - Loss: {metrics.avg_loss:.3f} | '\n",
    "          f'Acc: {metrics.accuracy:.3f}% ({int(metrics.accuracy * metrics.total_samples / 100)}/{metrics.total_samples}) | '\n",
    "          f'Conf: {metrics.avg_confidence:.2f}% | Time: {metrics.elapsed_time:.2f}s')\n",
    "\n",
    "    return metrics.accuracy, metrics.avg_confidence\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, criterion, data_loader: DataLoader, device: str) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate model on test data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (accuracy, average_confidence)\n",
    "    \"\"\"\n",
    "    metrics = run_epoch(model, criterion, data_loader, device, optimizer=None, is_training=False)\n",
    "\n",
    "    print(f'Test  - Loss: {metrics.avg_loss:.3f} | '\n",
    "          f'Acc: {metrics.accuracy:.3f}% ({int(metrics.accuracy * metrics.total_samples / 100)}/{metrics.total_samples}) | '\n",
    "          f'Conf: {metrics.avg_confidence:.2f}%')\n",
    "\n",
    "    return metrics.accuracy, metrics.avg_confidence\n",
    "\n",
    "\n",
    "def save_model(model: nn.Module, accuracy: float, config: Config) -> None:\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    config.checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'accuracy': accuracy,\n",
    "        'config': {\n",
    "            'embedding_dim': config.embedding_dim,\n",
    "            'num_classes': config.num_classes,\n",
    "            'dropout_rate_1': config.dropout_rate_1,\n",
    "            'dropout_rate_2': config.dropout_rate_2,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    save_path = config.checkpoint_dir / config.model_filename\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "\n",
    "def load_model(config: Config) -> EmbeddingClassifier:\n",
    "    \"\"\"Load model from checkpoint.\"\"\"\n",
    "    load_path = config.checkpoint_dir / config.model_filename\n",
    "\n",
    "    model = EmbeddingClassifier(config.embedding_dim, config.num_classes, config)\n",
    "    checkpoint = torch.load(load_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Model loaded from {load_path}\")\n",
    "    print(f\"Loaded model accuracy: {checkpoint['accuracy']:.2f}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_decision_boundary(\n",
    "    model: EmbeddingClassifier,\n",
    "    bounds: Tuple[float, float, float, float],\n",
    "    config: Config,\n",
    "    show_classes: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot decision boundary or confidence map in embedding space.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        bounds: (x_min, x_max, y_min, y_max) for plot region\n",
    "        config: Configuration object\n",
    "        show_classes: If True, show class assignments; if False, show confidence\n",
    "    \"\"\"\n",
    "    x_min, x_max, y_min, y_max = bounds\n",
    "\n",
    "    if not all(np.isfinite([x_min, x_max, y_min, y_max])):\n",
    "        print(\"Warning: Invalid bounds detected, using default range\")\n",
    "        x_min, x_max, y_min, y_max = -10, 10, -10, 10\n",
    "\n",
    "    if x_max <= x_min:\n",
    "        x_max = x_min + 10\n",
    "    if y_max <= y_min:\n",
    "        y_max = y_min + 10\n",
    "\n",
    "    x = np.arange(x_min, x_max, config.grid_resolution, dtype=np.float32)\n",
    "    y = np.arange(y_min, y_max, config.grid_resolution, dtype=np.float32)\n",
    "\n",
    "    if len(x) == 0 or len(y) == 0:\n",
    "        print(\"Warning: Empty grid, adjusting resolution\")\n",
    "        x = np.linspace(x_min, x_max, 50, dtype=np.float32)\n",
    "        y = np.linspace(y_min, y_max, 50, dtype=np.float32)\n",
    "\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "    # Create grid points for evaluation\n",
    "    grid_points = torch.from_numpy(\n",
    "        np.array([xx.ravel(), yy.ravel()]).T\n",
    "    ).float().to(config.device)\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        probabilities = torch.softmax(model.classifier(grid_points), dim=1)\n",
    "        probabilities = probabilities.cpu().numpy()\n",
    "\n",
    "    # Reshape for contour plotting\n",
    "    if show_classes:\n",
    "        class_assignments = probabilities.argmax(axis=1).reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, class_assignments, levels=config.num_classes, cmap='tab10', alpha=0.7)\n",
    "        plt.colorbar(label='Predicted Class')\n",
    "    else:\n",
    "        confidence_map = probabilities.max(axis=1).reshape(xx.shape)\n",
    "        contour = plt.contourf(xx, yy, confidence_map, levels=20, cmap='viridis', alpha=0.7)\n",
    "        plt.clim(0, 1)\n",
    "        plt.colorbar(contour, label='Max Confidence')\n",
    "\n",
    "    plt.axis('equal')\n",
    "\n",
    "\n",
    "def scatter_images_on_embeddings(\n",
    "    images: torch.Tensor,\n",
    "    embeddings: torch.Tensor,\n",
    "    config: Config\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Scatter actual images at their embedding coordinates.\n",
    "\n",
    "    Args:\n",
    "        images: Input images tensor\n",
    "        embeddings: Corresponding embedding coordinates\n",
    "        config: Configuration object\n",
    "    \"\"\"\n",
    "    num_samples = min(images.shape[0], config.viz_samples)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image = images[i].squeeze().cpu().numpy()\n",
    "        embedding_pos = (embeddings[i, 0].item(), embeddings[i, 1].item())\n",
    "\n",
    "        if not all(np.isfinite(embedding_pos)):\n",
    "            continue\n",
    "\n",
    "        offset_image = OffsetImage(image, cmap=\"gray\", zoom=config.viz_zoom)\n",
    "        annotation_box = AnnotationBbox(\n",
    "            offset_image, embedding_pos, xycoords='data', frameon=False, alpha=0.7\n",
    "        )\n",
    "        plt.gca().add_artist(annotation_box)\n",
    "\n",
    "\n",
    "def visualize_embedding_space(\n",
    "    model: EmbeddingClassifier,\n",
    "    data_loader: DataLoader,\n",
    "    config: Config,\n",
    "    title: str = \"Embedding Space Visualization\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of embedding space.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        data_loader: Data loader for visualization\n",
    "        config: Configuration object\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Get batch of data and embeddings\n",
    "    inputs, _ = next(iter(data_loader))\n",
    "    inputs = inputs.to(config.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.get_embeddings(inputs).cpu()\n",
    "\n",
    "    valid_embeddings = embeddings[torch.isfinite(embeddings).all(dim=1)]\n",
    "\n",
    "    if len(valid_embeddings) == 0:\n",
    "        print(\"Warning: No valid embeddings found, using default bounds\")\n",
    "        bounds = (-10, 10, -10, 10)\n",
    "    else:\n",
    "        margin = 3\n",
    "        x_vals = valid_embeddings[:, 0]\n",
    "        y_vals = valid_embeddings[:, 1]\n",
    "\n",
    "        bounds = (\n",
    "            float(x_vals.min() - margin),\n",
    "            float(x_vals.max() + margin),\n",
    "            float(y_vals.min() - margin),\n",
    "            float(y_vals.max() + margin)\n",
    "        )\n",
    "\n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plot_decision_boundary(model, bounds, config)\n",
    "    scatter_images_on_embeddings(inputs.cpu(), embeddings, config)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Embedding Dimension 1')\n",
    "    plt.ylabel('Embedding Dimension 2')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d90146b-527a-4670-aa59-61a16bf28273",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main training and evaluation pipeline.\"\"\"\n",
    "# Edit configuration here like so\n",
    "config = Config(\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "print(\"CNN Embedding Space Learning\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Using device: {config.device}\")\n",
    "\n",
    "# Create data loaders\n",
    "print(\"\\nPreparing MNIST data...\")\n",
    "mnist_train_loader, mnist_test_loader = create_data_loaders(datasets.MNIST, config)\n",
    "\n",
    "# Create and setup model\n",
    "print(\"Building model...\")\n",
    "model = EmbeddingClassifier(config.embedding_dim, config.num_classes, config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    momentum=config.momentum,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nTraining...\")\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    print(f'\\nEpoch {epoch + 1}/{config.epochs}:')\n",
    "    train_epoch(model, criterion, optimizer, mnist_train_loader, config.device)\n",
    "    test_acc, _ = evaluate_model(model, criterion, mnist_test_loader, config.device)\n",
    "\n",
    "    if test_acc > best_accuracy:\n",
    "        best_accuracy = test_acc\n",
    "\n",
    "# Save model\n",
    "save_model(model, best_accuracy, config)\n",
    "\n",
    "# Visualize MNIST embeddings\n",
    "print(\"\\nVisualizing MNIST embeddings...\")\n",
    "try:\n",
    "    visualize_embedding_space(model, mnist_test_loader, config, \"MNIST Embedding Space\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Visualization error: {e}\")\n",
    "\n",
    "# Domain transfer experiment\n",
    "print(\"\\nTesting domain transfer with Fashion-MNIST...\")\n",
    "fashion_train_loader, fashion_test_loader = create_data_loaders(datasets.FashionMNIST, config)\n",
    "\n",
    "fashion_acc, _ = evaluate_model(model, criterion, fashion_test_loader, config.device)\n",
    "\n",
    "# Visualize Fashion-MNIST embeddings\n",
    "try:\n",
    "    visualize_embedding_space(\n",
    "        model, fashion_test_loader, config,\n",
    "        \"Fashion-MNIST Embeddings (MNIST-trained Model)\"\n",
    "    )\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Visualization error: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nResults Summary:\")\n",
    "print(f\"MNIST Test Accuracy: {best_accuracy:.2f}%\")\n",
    "print(f\"Fashion-MNIST Accuracy: {fashion_acc:.2f}%\")\n",
    "print(f\"Domain Transfer Gap: {best_accuracy - fashion_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
